# Review of Tong *et al.* 2024

```code
@article{tong2024hybridoctree_hex,
  title={HybridOctree\_Hex: Hybrid octree-based adaptive all-hexahedral mesh generation with Jacobian control},
  author={Tong, Hua and Halilaj, Eni and Zhang, Yongjie Jessica},
  journal={Journal of Computational Science},
  volume={78},
  pages={102278},
  year={2024},
  doi     = {10.1016/j.jocs.2024.102278},
  url     = {https://doi.org/10.1016/j.jocs.2024.102278},
  publisher={Elsevier}
}
```

* GitHub [repo](https://github.com/CMU-CBML/HybridOctree_Hex)

## Octree **Initialization** with Feature Preservation

### 0. Target Surface

Input:

* **Target Surface:** A closed and manifold surface mesh composed of a 3D triangular elements.
  * See the [HybridOctree_Hex/input boundaries](https://github.com/CMU-CBML/HybridOctree_Hex/tree/main/input%20boundaries) folder for boundary raw files, e.g., `Cup1_tri.raw` ... `wrench_tri.raw`.
  * *Alternatively, might `.slt` files be avilable?*
  * For example, `bunny_tri.raw` (33,739 lines, 618kB) has the format (11,248 points, 22,490 elements):
  
```code
11248 22490  # line 1
0.121904 0.714156 0.831321  # line 2, coordinates
0.121904 0.714156 0.831321
0.943086 1.00012 1.6947
...
0.263885 1.15905 1.42424
0.389988 0.897909 1.43231
0.268144 0.793896 1.27816  # line 11249
7752 1374 1  # line 11250, connectivity
1 1374 10841
3831 7752 1
...
10983 11126 11129
11149 11061 11102
11220 11234 11103  # line 33739
```

Based on the [README.md](https://github.com/CMU-CBML/HybridOctree_Hex/blob/main/README.md), the resulting bunny hexahedral FEA mesh has 26,375 vertices, 21,695 elements, and a minimum scaled Jacobian of 0.57.

### 1. Octree: Refined, then Strongly Balanced

The octree is *refined* at regions of high curvature and narrow thickness.

* **Curvature Detection:** Gaussian curvature $G$ is calculated for surface points.  Five thresholds $\{0.5, 1, 2, 4, 8\}$ are used.  If a cell at level $l+4$ satisfies $G > G_{\rm{thresh}}[l]$, it is refined to level $l+5$.
* **Narrow Region Detection:** Thickness $T$ is measured via ray-casting.  If $T < T_{\rm{thresh}}$ (thresholds: $\{16, 8, 4, 2, 1\}$), the cell is refined.

The resulting octree levels for each octant typically range from **level 5 to 9**.

After the initialization, the octree is updated with the following two rules to ensure that the octree is **strongly balanced:**

* **Balancing Rule:** The level difference between neighboring octants must be at most one.
* **Paring Rule:** If an octant is subdivided to meet the balancing rule, its seven siblings must also be subdivded.

### 2. All-Hex Dual Mesh

Five **pre-defined 3D templates** are used to directly extract an all-hexahedral dual mesh.

* **Hanging Nodes:** These occur at transitions between different resolution levels.
* **Templates:** The system detects **transition faces** (shared by two cells of different levels) and **transition edges** (shared by three cells).
* The **5 Templates:** 
  * One (1) handles face transitions and 
  * Four (4) handle various edge transition configurations.
  * This ensures every grid point is shared by exactly eight polyhedra, resulting in a valid hex mesh.

The minimum scaled Jacobian for the templates is **0.258**, which is the starting point before Section 4 optimization.

### 3. Buffer Zone Clearing and Geometric Restriction

The **core mesh** is the subset of the dual mesh that fits completely within the target volume.  It is the largest possible assembly of dual hexes that fits entirely within the target volume while leaving a small gap (the **buffer zone**) for boundary alignment.
* The **core mesh** has a *blockly* (synonyms: *jagged, stair step, sugar cube*) appearance.
* The **core mesh** is created by removing elements from the dual mesh that lie outside or too close to the target surface.

A **boundary point** $x$ is any vertex that lies on the outer (quadrilateral) surface of the core mesh.

The **buffer zone** is the empty region between the jagged outer boundary of the core mesh and the target surface.

#### 3.1 Clearing

The **clearing** prcoess intentially removes elements that are potentially too close to the target surface, which could result in a lower quality element.

If we were to keep every hex that is technicall "inside" the target surface, some of the core hexes would have vertices that almost touch the target boundary.  Then, in later steps, where we connect these core vertices to the surface, a paper-thin (e.g., 1 percent thickness) element would be created, leaded to a bad scaled Jacobian, or even an inverted element if the vertex accidentally crosses the boundary during optimization.

Let

* $s_{\max}$ be the maximum size (typically as maximum edge length) of all elements sharing a vertex.
* size threshold $\epsilon_s := s_{\max}/2$.

Then,

* **Vertex clearing rule:** If the minimum distance between from a vertex to the boundary falls below the size threshold $\epsilon_s$, all elements sharing that vertex are deleted.

By setting the threshold to half of the size of the largest local element ($s_{\max}/2$), the algorithm ensures that the "gap" (the buffer zone) is of substantial size.  This half-size rule effectively "erodes" the core mesh until there is a guaranteed clearance.  It also ensures tha the final "buffer layer" hexes have a healthy aspect ratio (roughly $1:2$ or better) before the optimization beging.

In short, clearing eliminates interior volume of the core mesh to make room for high-quality boundary hexes.  Without this elimination step, the mesh would likely fail in thin or high-curvature regions.

##### Signed Distance Function (SDF)

The authors then noted, "During implementation, we observed that this setting [the vertex clearing rule] can be sensitive to large elements located in size transition regions, potentially leaving holes in the surface.  To address this issue, we calculate the signed distance function for corner points associated with every hext element.  Each hex had eight signed distance functions $f(\mathbf{x}_i)$, where $i=0, 1, 2 \ldots 7$.  We compute $f_{\min}$ and $f_{\max} and remove athe hex element if the condition $f_{\min} + 0.1 \times f_{\max} < 0$ is met."

To illustrate this problem image a large hex sitting next to small hexes (a size transition).  If a single vertex of the large hex is flagged as "too close" to the target surface, the vertex clearing rule forces the deletion of the entire large hex.  Because the hex is large, a massive chunk of the model's interior gets deleted, leaving behind the smaller neighboring hexes that likely are unable to "fill" the gap easily, resulting in a hole or a broken surface of the final mesh.

The authors replaced the brittle vertex clearing rule with the SDF reult.  While the vertex clearing rule removed everything based on a single vertex rule, the SDF considers all eight corners of a specific hex to decide if it should be eliminated or not.  

The weighted decision formula, $f_{\min} + 0.1 \times f_{\max} < 0$ acts as a soft-boundary filter.

Let

* $\mathcal{S}$ be a closed, orientable 2-manifold surface in 3D space.
  * A 2-manifold surface is a topological space that, at every point, looks locally like a small piece of the 2D Euclidean plane ($\mathbb{R}^2$).
* $\mathbf{p} \in \mathbb{R}^3$ be a query point (e.g., a corner of a hex element).
* $\mathbf{p}_s \in \mathcal{S}$ be the closest point on the surface $\mathcal{S}$ to $\mathbf{p}$.
* $\mathbf{n}_{\mathcal{S}}(\mathbf{p}_s)$ is the **inward-pointing** (not outward-pointing as is typically defined) unit normal vector of the surface $\mathcal{S}$ at the closest point $\mathbf{p}_s$.
* $\text{sgn}(x)$ is the signum function, returning $1$ if $x > 0$, $-1$ if $x < 0$, and $0$ if $x = 0$.

The Signed Distance Function (SDF) $f: \mathbb{R}^3 \to \mathbb{R}$ for $\mathcal{S}$ is defined as:

$$
f(\mathbf{p}) = 
\text{sgn}
\left[
    \mathbf{n}_{\mathcal{S}}(\mathbf{p}_s)
    \cdot
    (\mathbf{p} - \mathbf{p}_s)
\right] 
\|\mathbf{p} - \mathbf{p}_s \|
$$

where:

* The **magnitude** $\| \mathbf{p} - \mathbf{p}_s \|$ is the shortest Euclidean distance from the query point to the manifold.  It is always non-negative.
* The **direction vector** $(\mathbf{p} - \mathbf{p}_s)$ points from the closest point on the surface to the query point.
  * **Outside the surface:**
    * If the query point $\mathbf{p}$ lies outside of the surface, the vector $(\mathbf{p} - \mathbf{p}_s)$ points opposite of the surface inward normal ~~in the same general direction as the surface outward normal~~ $\mathbf{n}_{\mathcal{S}}$, making the dot product negative and $f < 0$.
  * **Inside the surface:**
    * If the query point $\mathbf{p}$ lies inside of the surface, the vector $(\mathbf{p} - \mathbf{p}_s)$ points in the same general direction as the surface inward normal $\mathbf{n}_{\mathcal{S}}$, making the dot product positive and $f > 0$.
  * **On the surface:**
    * If $\mathbf{p}$ is on the surface, $\mathbf{p} = \mathbf{p}_s$, the distance is $\mathbf{0}$, and the dot product is zero, thus $f=0$.

We remove a hex if $f_{\rm min} + 0.1 \times f_{\rm max} < 0$.

Note the definition of a signed distance must be reversed to keep the core and remove the outside.  There is a slight reversal in the standard mathematical definition of SDF and the removal formula in the paper.

If one uses the standard mathematical definition ($f > 0$ is outside) with the paper's formula ($f_{\min} + 0.1 \times f_{\max} < 0$), one will delete the interior of the model and keep the empty air around it.

**Sign Reversal:**  For the paper's criterion to work as intended (to keep "core" and remove "outside"), the paper must be used with the **material convention**:

* **Inside** $f > 0$ (positive distance into the material)
* **Outside** $f < 0$ (negative distance into the void)

###### Examples:

The following example test the logic of the "material convention" (positive = inside), and illustrate why $f_{\min} + 0.1 \times f_{\max} < 0$ is so clever when positive is inside.

* Scenario A: **Deeply Inside**
  * All 8 corner of the hex have $f \approx +10$.
  * $10 + 0.1(10) = 11$.  Since $11$ is **not** $< 0$, the hex is **retained**.
* Scenario B: **Deeply Outside**
  * All 8 corner of the hex have $f \approx -10$.
  * $-10 + 0.1(-10) = -11$.  Since $-11 < 0$ the hex is **removed**.
* Scenario C: **Straddling (but mostly outside)**
  * $f_{\min} = -5$ (far outside), $f_{\max} = +1$ (barely inside).
  * $-5 + 0.1(1) = -4.9$.  Since $-4.9 < 0$, the hex is **removed**.
* Scenario D: **Straddling (but mostly inside)**
  * $f_{\min} = -0.5$ (barely outside), $f_{\max} = +10$ (deeply inside).
  * $-0.5 + 0.1(10) = +0.5$.  Since 0.5 is **not** $< 0$, the hex is **retained**.

**Insight:** The foregoing "weigthed" rule allows a hex to stay even if a corner poke slightly out, as long as the rest of the hext is deeply buried inside.  This is exactly what prevents "holes" in size-transition regions.

#### 3.2 Restriction

Simply having the valid core mesh is not sufficient:
* If angles between the faces surrounding a boundary point $x$ are too sharp, the new hexes created in the buffer zone will be *inverted* or *of extremely poor quality*.

Boundary point $x$ to target surface $x_s$

* **Connectivity:** A boundary point $x$ is shared by $m$ quadrilateral faces that form the external surface of the core mesh.
* **Role in Meshing:** Every boundary point $x$ is eventually connected to its closest point on the target surface $x_s$ via an edge vector $e$.  This connection ~~"stretches"~~ **augments** the mesh to fill in the buffer zone.

The **normal vector** $n_i$ is defined as the normal of a triangle formed by boundary point $x$ and two of its adjacent boundary points.

The **Restriction:** To prevent poor-quality elements when connecting to the target surface, a normal-based restriction is enforced.  For a boundary point $x$, any three normals ($n_i$, $n_j$, $n_k$) of the surrounding faces must satisfy $\left( n_i \times n_j \right) \cdot n_k > 0$.
* **Iterative Removal:** Hexes are deleted one-by-one until all boundary points satisfy this geometric restriction.
* This restriction guarantees that the remaining boundary points $x$ have a geometry that guarantees a scaled Jacobian $> 0$ for the final boundary elements.
* **Deletion Priority:** The algorithm priorizes hexes with the **highest number of boundary faces** during buffer clearing to avoid creating internal holes.

### 4. Quality Improvement with Jacobian Control

The final step meshes the **buffer zone** by connecting core boundary points $x$ to their closest surface points $x_s$ and optimizing the resulting elements.

* **Smart Laplacian Smoothing:** Performed every 1,000 iterations on the outmost two layers to speed up convergence.
* **Energy Minimization:** A gradient-based method minimizes an energy function $E$:

$E = E_{\rm GF}({\rm Geometry\;Fitting}) - E_{\rm{SJ}}({\rm Scaled\;Jacobian}) - E_{\rm J}(\rm{Jacobian})$

* **Jacobian Control:** Because the **Scaled Jacobian** is non-differentiuable in certain regions, the algorithm switches to the **Jacobian** term for negative-Jacobian elements to ensure they can be *untangled*.
* The paper specifically identifies $E_{\rm J}$ (the Jacobian term) as the mechanism to **untangle** elements with negative Jacobians.
* Using a combinated scaled Jacobian and Jacobian helps the optimizer not to get stuck in local minima.
* The noval buffer zone clearance and mesh quality enhancements lead to significantly higher minimum scaled Jacobian $(> 0.5)$.